{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**summarize**\n",
    "- use MapReduce kind of logic\n",
    "\n",
    "**question-answer**\n",
    "- use retrieval qa chain kind of logic\n",
    "\n",
    "**troubleshoot**\n",
    "- use a retriever -> something similar to web-researcher\n",
    "    - internet-search\n",
    "    - internal-rag-search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [item[\"@message\"] for item in results]\n",
    "messages = messages[::-1] # reason - list is ordered in desc based on timestamp in query\n",
    "text = \"\\n\".join(messages)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an expert in understanding logs from an application. \n",
      "Summarize the given log data in bullet points outline the main issue. Make a markdown table of error and source.\n",
      "Logs: \u001b[33;1m\u001b[1;3m{text}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "summarize_prompt_template = \"\"\"You are an expert in understanding logs from an application. \n",
    "Summarize the given log data in bullet points outline the main issue. Make a markdown table of error and source.\n",
    "Logs: {text}\"\"\"\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate.from_template(summarize_prompt_template)\n",
    "\n",
    "summarize_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    # model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = summarize_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = summarize_chain.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided log data, the main issue appears to be a CUDA kernel error with the message \"CUDA error: invalid argument\". Here's a summary of the key points:\n",
      "\n",
      "**Outline:**\n",
      "\n",
      "- The CUDA kernel error is reported asynchronously, so the provided stacktrace might not be accurate.\n",
      "- The error occurred during the `Decode` method of the `text_generation_server.server.Server` class.\n",
      "- The error occurred within the `forward` method of the `text_generation_server.models.custom_modeling.flash_rw_modeling.FlashRWTransformer` class.\n",
      "- The error occurred when trying to apply the activation function (`self.act`) to the hidden states.\n",
      "\n",
      "**Markdown Table of Error and Source:**\n",
      "\n",
      "| Error | Source |\n",
      "| --- | --- |\n",
      "| RuntimeError: CUDA error: invalid argument | `text_generation_server.models.custom_modeling.flash_rw_modeling.FlashRWTransformer.forward` |\n",
      "\n",
      "The log data suggests two potential solutions to debug the issue:\n",
      "\n",
      "1. Compile the application with `TORCH_USE_CUDA_DSA` to enable device-side assertions, which may provide more information about the error.\n",
      "2. Run the application with `CUDA_LAUNCH_BLOCKING=1` to force synchronous CUDA kernel execution, which may help with the asynchronous reporting of the error.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided log data, the main issue appears to be a CUDA kernel error with the message \"CUDA error: invalid argument\". Here\\'s a summary of the key points:\\n\\n**Outline:**\\n\\n- The CUDA kernel error is reported asynchronously, so the provided stacktrace might not be accurate.\\n- The error occurred during the `Decode` method of the `text_generation_server.server.Server` class.\\n- The error occurred within the `forward` method of the `text_generation_server.models.custom_modeling.flash_rw_modeling.FlashRWTransformer` class.\\n- The error occurred when trying to apply the activation function (`self.act`) to the hidden states.\\n\\n**Markdown Table of Error and Source:**\\n\\n| Error | Source |\\n| --- | --- |\\n| RuntimeError: CUDA error: invalid argument | `text_generation_server.models.custom_modeling.flash_rw_modeling.FlashRWTransformer.forward` |\\n\\nThe log data suggests two potential solutions to debug the issue:\\n\\n1. Compile the application with `TORCH_USE_CUDA_DSA` to enable device-side assertions, which may provide more information about the error.\\n2. Run the application with `CUDA_LAUNCH_BLOCKING=1` to force synchronous CUDA kernel execution, which may help with the asynchronous reporting of the error.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "display(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question-answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
