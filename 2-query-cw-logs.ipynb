{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3, json, time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('logs')\n",
    "\n",
    "last_x_days = 300\n",
    "log_group_names = [\n",
    "    \"/aws/sagemaker/Endpoints/amazon-falconlite2-2023-10-07-18-07-48-251\", \n",
    "    # \"/aws/sagemaker/Endpoints/MistralLite-2023-10-19-09-58-03\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cw_logs(query, last_x_days=last_x_days, log_group_names=log_group_names):\n",
    "    last_time = timedelta(days=last_x_days)\n",
    "\n",
    "    start_query_response = client.start_query(\n",
    "        logGroupNames=log_group_names,\n",
    "        startTime=int((datetime.today() - last_time).timestamp()),\n",
    "        endTime=int(datetime.now().timestamp()),\n",
    "        queryString=query,\n",
    "    )\n",
    "    \n",
    "    query_id = start_query_response['queryId']\n",
    "    response = None\n",
    "    while response == None or response['status'] == 'Running':\n",
    "        print('Waiting 2 secs for query to complete ...')\n",
    "        time.sleep(2)\n",
    "        response = client.get_query_results(\n",
    "            queryId=query_id\n",
    "        )\n",
    "    print('Done!')\n",
    "    # response.keys(): dict_keys(['results', 'statistics', 'status', 'ResponseMetadata'])\n",
    "    results_formatted = [{item[\"field\"]: item[\"value\"] for item in list_dict} for list_dict in response[\"results\"]]\n",
    "    response[\"results\"] = results_formatted\n",
    "    return response \n",
    "    # data = []\n",
    "    # for item in response[\"results\"]:\n",
    "    #     new_item = {}\n",
    "    #     for record in item:\n",
    "    #         new_item[record[\"field\"]] = record[\"value\"]\n",
    "    #     data.append(new_item)\n",
    "    \n",
    "    # df = pd.DataFrame(data)\n",
    "    # return df\n",
    "\n",
    "# TODO: have the llm return - query, time_period, log_groups as well\n",
    "# df = query_cw_logs(query, last_x_days=300)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all records with ERROR sort with timestamp and log_group_id\n",
    "# query = \"\"\"\n",
    "# fields @message \n",
    "# | filter @message like /ERROR/\n",
    "# | parse @message \"* : *\" as loggingRandom, loggingMessage\n",
    "# | display @timestamp, loggingMessage\n",
    "# | sort @timestamp \n",
    "# \"\"\"\n",
    "\n",
    "# Extracts the fields loggingTime, loggingType and loggingMessage, filters down to log events that contain ERROR or INFO strings, and then displays only the loggingMessage and loggingType fields for events that contain an ERROR string.\n",
    "# query = \"\"\"\n",
    "# fields @message\n",
    "#     | parse @message \"* [*] *\" as loggingTime, loggingType, loggingMessage\n",
    "#     | filter loggingType IN [\"ERROR\", \"INFO\"]\n",
    "#     | display loggingMessage, loggingType = \"ERROR\" as isError\n",
    "# \"\"\"\n",
    "\n",
    "# \"show the latest 20 log events\",\n",
    "query = \"\"\"fields @timestamp, @message\n",
    "| sort @timestamp desc\n",
    "| limit 20\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 2 secs for query to complete ...\n",
      "Waiting 2 secs for query to complete ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "response = query_cw_logs(query, last_x_days=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@timestamp': '2023-10-07 18:16:51.277',\n",
       " '@message': 'Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.',\n",
       " '@ptr': 'CpQBClQKUDMyNDYyMjQwMDUxNDovYXdzL3NhZ2VtYWtlci9FbmRwb2ludHMvYW1hem9uLWZhbGNvbmxpdGUyLTIwMjMtMTAtMDctMTgtMDctNDgtMjUxEAQSOBoYAgZPOTnzAAAAAnuR8+YABlIaBBAAAAAyIAEo5IXo2rAxMM3+7NqwMTg5QLnTCUjbpQFQh6IBGAAgARA4GAE='}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response.keys()\n",
    "\n",
    "response[\"results\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "RuntimeError: CUDA error: invalid argument\n",
      "> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\", line 21, in intercept\n",
      "    return await response\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\", line 82, in _unary_interceptor\n",
      "    raise error\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\", line 73, in _unary_interceptor\n",
      "    return await behavior(request_or_iterator, context)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\", line 121, in Decode\n",
      "    generations, next_batch = self.model.generate_token(batch)\n",
      "  File \"/opt/conda/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 753, in generate_token\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 750, in generate_token\n",
      "    out = self.forward(batch)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 717, in forward\n",
      "    return self.model.forward(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 643, in forward\n",
      "    hidden_states = self.transformer(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 603, in forward\n",
      "    hidden_states, residual = layer(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 534, in forward\n",
      "    mlp_output = self.mlp(ln_mlp)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 372, in forward\n",
      "    hidden_states = self.act(hidden_states)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n",
      "    sys.exit(app())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 311, in __call__\n",
      "    return get_command(self)(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line 83, in serve\n",
      "    server.serve(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\", line 207, in serve\n",
      "    asyncio.run(\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\", line 159, in invoke_intercept_method\n",
      "    return await self.intercept(\n",
      "#033[2m2023-10-07T18:16:46.582609Z#033[0m #033[31mERROR#033[0m #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Method Decode encountered an error.\n",
      "> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\", line 21, in intercept\n",
      "    return await response\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\", line 82, in _unary_interceptor\n",
      "    raise error\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\", line 73, in _unary_interceptor\n",
      "    return await behavior(request_or_iterator, context)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\", line 121, in Decode\n",
      "    generations, next_batch = self.model.generate_token(batch)\n",
      "  File \"/opt/conda/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 753, in generate_token\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 750, in generate_token\n",
      "    out = self.forward(batch)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 717, in forward\n",
      "    return self.model.forward(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 643, in forward\n",
      "    hidden_states = self.transformer(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 603, in forward\n",
      "    hidden_states, residual = layer(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 534, in forward\n",
      "    mlp_output = self.mlp(ln_mlp)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 372, in forward\n",
      "    hidden_states = self.act(hidden_states)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n",
      "    sys.exit(app())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 311, in __call__\n",
      "    return get_command(self)(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line 83, in serve\n",
      "    server.serve(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\", line 207, in serve\n",
      "    asyncio.run(\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\", line 159, in invoke_intercept_method\n",
      "    return await self.intercept(\n",
      "#033[2m2023-10-07T18:16:46.582652Z#033[0m #033[31mERROR#033[0m #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Method Decode encountered an error.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "#033[2m2023-10-07T18:16:46.582175Z#033[0m #033[31mERROR#033[0m #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Method Decode encountered an error.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n",
      "    sys.exit(app())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 311, in __call__\n",
      "    return get_command(self)(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line 83, in serve\n",
      "    server.serve(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\", line 207, in serve\n",
      "    asyncio.run(\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\", line 159, in invoke_intercept_method\n",
      "    return await self.intercept(\n",
      "> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\", line 21, in intercept\n",
      "    return await response\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\", line 82, in _unary_interceptor\n",
      "    raise error\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\", line 73, in _unary_interceptor\n",
      "    return await behavior(request_or_iterator, context)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\", line 121, in Decode\n",
      "    generations, next_batch = self.model.generate_token(batch)\n",
      "  File \"/opt/conda/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 753, in generate_token\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 750, in generate_token\n",
      "    out = self.forward(batch)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\", line 717, in forward\n",
      "    return self.model.forward(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 643, in forward\n",
      "    hidden_states = self.transformer(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 603, in forward\n",
      "    hidden_states, residual = layer(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 534, in forward\n",
      "    mlp_output = self.mlp(ln_mlp)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\", line 372, in forward\n",
      "    hidden_states = self.act(hidden_states)\n",
      "RuntimeError: CUDA error: invalid argument\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n"
     ]
    }
   ],
   "source": [
    "for item in response[\"results\"]:\n",
    "    print(item[\"@message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'results' (list)\n"
     ]
    }
   ],
   "source": [
    "results = response[\"results\"]\n",
    "%store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
